{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdf2de0",
   "metadata": {},
   "source": [
    "**If training on colab, be sure to use a GPU (runtime > Change runtime type > GPU)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3483926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run the lines below if running in google colab\n",
    "# !pip install tensorflow==2.4.3\n",
    "# !git clone https://github.com/jlaihong/image-super-resolution.git\n",
    "# !mv image-super-resolution/* ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab2bcb",
   "metadata": {},
   "source": [
    "# SRResNet and SRGAN Training for Image Super Resolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c4a230b",
   "metadata": {},
   "source": [
    "- using AI to improve the quality of images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5063dc80",
   "metadata": {},
   "source": [
    "## Background and Intuition\n",
    "- SRResNet and SRGAN\n",
    "  - fall under an area of research called image super-resolution\n",
    "  - both introduced in a paper showing and explaining some implementation: \n",
    "    - [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e1597e6",
   "metadata": {},
   "source": [
    "- here is the code used to train both of these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8fc6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, MeanAbsoluteError\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from PIL import Image\n",
    "\n",
    "from datasets.div2k.parameters import Div2kParameters \n",
    "from datasets.div2k.loader import create_training_and_validation_datasets\n",
    "from utils.dataset_mappings import random_crop, random_flip, random_rotate, random_lr_jpeg_noise\n",
    "from utils.metrics import psnr_metric\n",
    "from utils.config import config\n",
    "from utils.callbacks import SaveCustomCheckpoint\n",
    "from models.srresnet import build_srresnet\n",
    "from models.srgan import build_discriminator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1761209",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "- we have to show the models some example of low res and corresponding high res images.\n",
    "- in this case its easy, we can use some high res images and we can resisze them to be low res images\n",
    "- the paper uses a random sampel of 350k samples from the image net database\n",
    "  - 150 GB of images and not all of the images are high quality \n",
    "- were using the DIV2k dataset since its high quality and only 4 GB\n",
    "  - 2k resultion\n",
    "  - specifically designed for image super resolution\n",
    "  - has specific sets of low res images that corresponds to high resolution data\n",
    "  - can read descriptions here: \n",
    "    - https://data.vision.ee.ethz.ch/cvl/DIV2K/\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a4ad9b0",
   "metadata": {},
   "source": [
    "- According to the author, it should be fairly easy to train with other datasets\n",
    "- we just need to change the dataset key\n",
    "- DivkParameters handles downloading and parsing the div2k dataset\n",
    "- depending on the dataset you choose it will determine the scaling factor\n",
    "  - any of the 2k datasets uses a scaling factor of x2, and of the x4 datasets uses a scaling factor of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9272c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_key = \"bicubic_x4\"\n",
    "\n",
    "data_path = config.get(\"data_path\", \"\") \n",
    "\n",
    "div2k_folder = os.path.abspath(os.path.join(data_path, \"div2k\"))\n",
    "\n",
    "dataset_parameters = Div2kParameters(dataset_key, save_data_directory=div2k_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2629a373",
   "metadata": {},
   "source": [
    "- we can't just send whole images to the network\n",
    "- the images are really big, the GPU/CPU could run our of memory\n",
    "- we crop patches out of the images and send those to the model instead\n",
    "- the paper uses high res crops of 96x96 pixels\n",
    "- se we do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639c8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_crop_size = 96"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c1d4aad",
   "metadata": {},
   "source": [
    "### defining a list of mappings to use during training\n",
    "- this first mapping takes crops of 96 by 96 from the high res images and the corresponding scale of the appropriate low res image from the low res image\n",
    "- eg x4 means the images are downscaled by 4 times\n",
    "  - the low res patches will be 24 by 24\n",
    "- data augmentation\n",
    "- horizontal filpping and rotation\n",
    "- not mentioned in the paper\n",
    "- its included since our dataset is a lot smaller than the papers\n",
    "- this model is also trained with jpeg noise so the model learns to remove that as well\n",
    "- you can read through the code in the div2k `loader.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d48fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mappings = [\n",
    "    lambda lr, hr: random_crop(lr, hr, hr_crop_size=hr_crop_size, scale=dataset_parameters.scale), \n",
    "    random_flip, \n",
    "    random_rotate, \n",
    "    random_lr_jpeg_noise]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5198f817",
   "metadata": {},
   "source": [
    "-create interflow data set objects\n",
    "- dataset objects are optimized to cache the images in the dataset to perform the calculations faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "850ab510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 14:38:25.660505: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = create_training_and_validation_datasets(dataset_parameters, train_mappings)\n",
    "\n",
    "valid_dataset_subset = valid_dataset.take(10) # only taking 10 examples here to speed up evaluations during training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "894b92c5",
   "metadata": {},
   "source": [
    "## Train the SRResNet generator model\n",
    "\n",
    "- defining the srresnet method\n",
    "- following the architecture given in the paper\n",
    "- if you've worked with CNNs before, you'll notice the width and height dimensions decrease as we get deeper into the network\n",
    "- this is because we use pooling layers or apply filters with strides greater than 1\n",
    "- but for images super resolution we don't want the width and heigth resolution to decrease\n",
    "  - we want the **opposite**\n",
    "- so the srresnet doesnt use pooling layers, and only uses strides of 1 and always uses 1\n",
    "- this model is restricted to upsampling of 2, 4, or 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8149e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_srresnet(scale=dataset_parameters.scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbe4eb30",
   "metadata": {},
   "source": [
    "## SRResNet architecture\n",
    "- built using structure from (paper)[https://arxiv.org/abs/1609.04802]\n",
    "\n",
    "```python\n",
    "def build_srresnet(scale=4, num_filters=64, num_res_blocks=16):\n",
    "    if scale not in upsamples_per_scale:\n",
    "        raise ValueError(f\"available scales are: {upsamples_per_scale.keys()}\")\n",
    "\n",
    "    num_upsamples = upsamples_per_scale[scale]\n",
    "```\n",
    "### Start by defining an input layer\n",
    "\n",
    "- `(None, None 3)` to take color images of any height and width\n",
    "  - the network is fully convolution's network, so we're able to do that (not worry about size of the images)\n",
    "- input images contain pixel values between 0 and 255\n",
    "  - in the paper the scale is between 0 and 1\n",
    "- so we do the same, \n",
    "  - data normalization is usually done outside the network\n",
    "  - this is done inside the model for convenience, \n",
    "  - now anyone using the model doesn't have to worry about normalizing it before sending it into the model and if they forget to normalizing you'd get strange results\n",
    "```python\n",
    "\n",
    "    lr = Input(shape=(None, None, 3))\n",
    "    x = Lambda(normalize_01)(lr)\n",
    "```\n",
    "    # uses 64 filters of size 9x9 since we dont want the hight and width to decrease we use same padding\n",
    "```python\n",
    "    x = Conv2D(num_filters, kernel_size=9, padding='same')(x)\n",
    "```\n",
    "### Parametric ReLu \n",
    "- Neural networks use no linear activation functions to do complex mappings\n",
    "  - in our case we are trying to learn the mapping from low res images to high res images\n",
    "  - in practice the normal ReLu function still works well\n",
    "  -  the problem is it causes a lot of dead neurons inside your network because every number less than 0 will simply be mapped to 0\n",
    "  - the parameters attached to those neurons are not included!\n",
    "- Leaky Relu\n",
    "  - instead of setting the values to 0 we set them ot some predefined constant fraction of themselves\n",
    "- Parameteric Relu \n",
    "  - we allow the network to edit that fraction instead, \n",
    "  - usually this would create a separate parameter value /alpha for every value that was passed into the layer\n",
    "- here we're saying the shared axes is equal to `[1, 2]` \n",
    "  - meaning that we're sharing the parameters across the width and hight parameters so we're only creating one parameter per channel. \n",
    "  - since there are 64 pulses there are only 64 channels so there are only 64 parameters\n",
    "  - we're sharing the parameters across the width and height dimensions\n",
    "  - the ouput of this PReLu Layer is also sent to multiple places so we've defined another variable `x_1` to make use of them later\n",
    "```python\n",
    "    x = x_1 = PReLU(shared_axes=[1, 2])(x)\n",
    "```\n",
    "### Residual block\n",
    "- we're just grouping common operations and calling it a residual block\n",
    "- all of the operations in each of the blocks are the same, \n",
    "- each block uses a convolutional layer 64 pulses of size 3x3 \n",
    "- followed by a batch normalization layer followed by a parametric relu followed by another convolutional layer with the same settings, \n",
    "- then another batch norm.\n",
    "- we take this batch norm and preform an element-wise sum with the input of the resudual block\n",
    "  - this is the the second time that the output of this relu is used\n",
    "- we continue staking residual blocks on top of the other, each time sending the output from the previous block as the input to the next block\n",
    "  - performing an element wise sum between the block input and its final batch nomr layer\n",
    "\n",
    "- this functino is responsitble for all of them! \n",
    "- see in `srresnet.py`\n",
    "```python\n",
    "def residual_block(block_input, num_filters, momentum=0.8):\n",
    "    x = Conv2D(num_filters, kernel_size=3, padding='same')(block_input)\n",
    "    x = BatchNormalization(momentum=momentum)(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization(momentum=momentum)(x)\n",
    "    x = Add()([block_input, x])\n",
    "    return x\n",
    "for _ in range(num_res_blocks):\n",
    "    x = residual_block(x, num_filters)\n",
    "\n",
    "```\n",
    "- the purpose of all of these resudual block is to extract features from the input image\n",
    "  - people have figured out that earlier layers in a CNN learns to detect low level features (like lines in a certain direction) and later layers detect shapes\n",
    "    - as you go deeper the layers learn more object features\n",
    "    - this really isn't a 1 to 1 comparison for super resolution but\n",
    "    - adding more residual layers produces better images\n",
    "    - comes with the trade off that your network will take longer to process images\n",
    "\n",
    "- after all of the residual blocks we have another convolutional layer and a batch norm layer\n",
    "- and we see the 3rd time that the output of the parametric relu is used\n",
    "- we preform an elementwise sum on that with the output layer\n",
    "```python\n",
    "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x_1, x])\n",
    "```\n",
    "- up until this point the height and width have stayed consistent through the network\n",
    "- now wer reach a new type of block\n",
    "- we call it the upsample block\n",
    "- uses an convolutional layer with 256 filter of size 3x3 so thats 4 times the previous layer\n",
    "- also a pixel shuffle layer\n",
    "  - this takes values from the channel dimension and sticks them into the height and width dimension\n",
    "  - so it doubles the height and with and divides the channel dimension by 4\n",
    "```python\n",
    "    for _ in range(num_upsamples):\n",
    "        x = upsample(x, num_filters * 4)\n",
    "\n",
    "    x = Conv2D(3, kernel_size=9, padding='same', activation='tanh')(x)\n",
    "    sr = Lambda(denormalize_m11)(x)\n",
    "\n",
    "    return Model(lr, sr)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir=f'./ckpt/sr_resnet_{dataset_key}'\n",
    "\n",
    "learning_rate=1e-4\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(0),\n",
    "                                 epoch=tf.Variable(0),\n",
    "                                 psnr=tf.Variable(0.0),\n",
    "                                 optimizer=Adam(learning_rate),\n",
    "                                 model=generator)\n",
    "\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
    "                                                directory=checkpoint_dir,\n",
    "                                                max_to_keep=3)\n",
    "\n",
    "if checkpoint_manager.latest_checkpoint:\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "    print(f'Model restored from checkpoint at step {checkpoint.step.numpy()} with validation PSNR {checkpoint.psnr.numpy()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 1_000_000\n",
    "\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "training_epochs = training_steps / steps_per_epoch\n",
    "\n",
    "if checkpoint.epoch.numpy() < training_epochs:\n",
    "    remaining_epochs = int(training_epochs - checkpoint.epoch.numpy())\n",
    "    print(f\"Continuing Training from epoch {checkpoint.epoch.numpy()}. Remaining epochs: {remaining_epochs}.\")\n",
    "    save_checkpoint_callback = SaveCustomCheckpoint(checkpoint_manager, steps_per_epoch)\n",
    "    checkpoint.model.compile(optimizer=checkpoint.optimizer, loss=MeanSquaredError(), metrics=[psnr_metric])\n",
    "    checkpoint.model.fit(train_dataset,validation_data=valid_dataset_subset, steps_per_epoch=steps_per_epoch, epochs=remaining_epochs, callbacks=[save_checkpoint_callback])\n",
    "else:\n",
    "    print(\"Training already completed. To continue training, increase the number of training steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_directory = f\"weights/srresnet_{dataset_key}\"\n",
    "os.makedirs(weights_directory, exist_ok=True)\n",
    "weights_file = f'{weights_directory}/generator.h5'\n",
    "checkpoint.model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603dc7a",
   "metadata": {},
   "source": [
    "## Train SRGAN using SRResNet as the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_srresnet(scale=dataset_parameters.scale)\n",
    "generator.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(hr_crop_size=hr_crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efb60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_5_4 = 20\n",
    "vgg = VGG19(input_shape=(None, None, 3), include_top=False)\n",
    "perceptual_model = Model(vgg.input, vgg.layers[layer_5_4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9008cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy = BinaryCrossentropy()\n",
    "mean_squared_error = MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=PiecewiseConstantDecay(boundaries=[100000], values=[1e-4, 1e-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba83996",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = Adam(learning_rate=learning_rate)\n",
    "discriminator_optimizer = Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgan_checkpoint_dir=f'./ckpt/srgan_{dataset_key}'\n",
    "\n",
    "srgan_checkpoint = tf.train.Checkpoint(step=tf.Variable(0),\n",
    "                                       psnr=tf.Variable(0.0),\n",
    "                                       generator_optimizer=Adam(learning_rate),\n",
    "                                       discriminator_optimizer=Adam(learning_rate),\n",
    "                                       generator=generator,\n",
    "                                       discriminator=discriminator)\n",
    "\n",
    "srgan_checkpoint_manager = tf.train.CheckpointManager(checkpoint=srgan_checkpoint,\n",
    "                                                directory=srgan_checkpoint_dir,\n",
    "                                                max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if srgan_checkpoint_manager.latest_checkpoint:\n",
    "    srgan_checkpoint.restore(srgan_checkpoint_manager.latest_checkpoint)\n",
    "    print(f'Model restored from checkpoint at step {srgan_checkpoint.step.numpy()} with validation PSNR {srgan_checkpoint.psnr.numpy()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(lr, hr):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        lr = tf.cast(lr, tf.float32)\n",
    "        hr = tf.cast(hr, tf.float32)\n",
    "\n",
    "        sr = srgan_checkpoint.generator(lr, training=True)\n",
    "\n",
    "        hr_output = srgan_checkpoint.discriminator(hr, training=True)\n",
    "        sr_output = srgan_checkpoint.discriminator(sr, training=True)\n",
    "\n",
    "        con_loss = calculate_content_loss(hr, sr)\n",
    "        gen_loss = calculate_generator_loss(sr_output)\n",
    "        perc_loss = con_loss + 0.001 * gen_loss\n",
    "        disc_loss = calculate_discriminator_loss(hr_output, sr_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(perc_loss, srgan_checkpoint.generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, srgan_checkpoint.discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, srgan_checkpoint.generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, srgan_checkpoint.discriminator.trainable_variables))\n",
    "\n",
    "    return perc_loss, disc_loss\n",
    "\n",
    "@tf.function\n",
    "def calculate_content_loss(hr, sr):\n",
    "    sr = preprocess_input(sr)\n",
    "    hr = preprocess_input(hr)\n",
    "    sr_features = perceptual_model(sr) / 12.75\n",
    "    hr_features = perceptual_model(hr) / 12.75\n",
    "    return mean_squared_error(hr_features, sr_features)\n",
    "\n",
    "def calculate_generator_loss(sr_out):\n",
    "    return binary_cross_entropy(tf.ones_like(sr_out), sr_out)\n",
    "\n",
    "def calculate_discriminator_loss(hr_out, sr_out):\n",
    "    hr_loss = binary_cross_entropy(tf.ones_like(hr_out), hr_out)\n",
    "    sr_loss = binary_cross_entropy(tf.zeros_like(sr_out), sr_out)\n",
    "    return hr_loss + sr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c41e85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perceptual_loss_metric = Mean()\n",
    "discriminator_loss_metric = Mean()\n",
    "\n",
    "step = srgan_checkpoint.step.numpy()\n",
    "steps = 200000\n",
    "\n",
    "monitor_folder = f\"monitor_training/srgan_{dataset_key}\"\n",
    "os.makedirs(monitor_folder, exist_ok=True)\n",
    "\n",
    "now = time.perf_counter()\n",
    "\n",
    "for lr, hr in train_dataset.take(steps - step):\n",
    "    srgan_checkpoint.step.assign_add(1)\n",
    "    step = srgan_checkpoint.step.numpy()\n",
    "\n",
    "    perceptual_loss, discriminator_loss = train_step(lr, hr)\n",
    "    perceptual_loss_metric(perceptual_loss)\n",
    "    discriminator_loss_metric(discriminator_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        psnr_values = []\n",
    "        \n",
    "        for lr, hr in valid_dataset_subset:\n",
    "            sr = srgan_checkpoint.generator.predict(lr)[0]\n",
    "            sr = tf.clip_by_value(sr, 0, 255)\n",
    "            sr = tf.round(sr)\n",
    "            sr = tf.cast(sr, tf.uint8)\n",
    "            \n",
    "            psnr_value = psnr_metric(hr, sr)[0]\n",
    "            psnr_values.append(psnr_value)\n",
    "            psnr = tf.reduce_mean(psnr_values)\n",
    "            \n",
    "        image = Image.fromarray(sr.numpy())\n",
    "        image.save(f\"{monitor_folder}/{step}.png\" )\n",
    "        \n",
    "        duration = time.perf_counter() - now\n",
    "        \n",
    "        now = time.perf_counter()\n",
    "        \n",
    "        print(f'{step}/{steps}, psnr = {psnr}, perceptual loss = {perceptual_loss_metric.result():.4f}, discriminator loss = {discriminator_loss_metric.result():.4f} ({duration:.2f}s)')\n",
    "        \n",
    "        perceptual_loss_metric.reset_states()\n",
    "        discriminator_loss_metric.reset_states()\n",
    "        \n",
    "        srgan_checkpoint.psnr.assign(psnr)\n",
    "        srgan_checkpoint_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbb0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_directory = f\"weights/srgan_{dataset_key}\"\n",
    "os.makedirs(weights_directory, exist_ok=True)\n",
    "weights_file = f'{weights_directory}/generator.h5'\n",
    "srgan_checkpoint.generator.save_weights(weights_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
